# üìö LLM Bare-Metal Project Index

**Last Updated**: November 20, 2025  
**Status**: ‚úÖ LLaMA2 Port COMPLETE (15M params on bare-metal EFI)

---

## üéØ Quick Start

**For Reviewers**: Read `SUMMARY_FOR_JUSTINE.md` first (5 min)  
**For Developers**: Read `README_LLAMA2.md` (10 min)  
**For Building**: See `Makefile` targets below

---

## üìÇ Project Structure

### üöÄ **Main Implementation (LLaMA2 Port)**

| File | Lines | Purpose | Status |
|------|-------|---------|--------|
| **llama2_efi.c** | 674 | LLaMA2 on bare-metal (95% Karpathy) | ‚úÖ COMPLETE |
| **llama2.efi** | - | Compiled EFI binary (18.8MB) | ‚úÖ READY |
| **stories15M.bin** | - | Model weights (60MB, 15M params) | ‚úÖ DOWNLOADED |
| **tokenizer.bin** | - | BPE tokenizer (32K vocab) | ‚úÖ READY |
| **llama2-disk.img** | - | Bootable disk with model (128MB) | ‚úÖ BUILT |

### üìñ **Documentation (Read These)**

#### **For Justine / Reviewers**
- **SUMMARY_FOR_JUSTINE.md** ‚≠ê - Quick summary (5 min read)
  - What was asked: "Use llama2.c, steal as much as you can"
  - What was built: 95% code reuse, 6 layers, 15M params
  - Win-win-win: Karpathy + Meta + Us
  
#### **For Developers**
- **README_LLAMA2.md** ‚≠ê - Complete guide
  - Build instructions
  - Code structure (what changed vs unchanged)
  - Testing in QEMU
  - Attribution details

- **LLAMA2_PORT_COMPLETE.md** - Detailed status report
  - Compilation results
  - Memory layout
  - Before/after comparison
  - Next steps

#### **Strategy & Planning**
- **MAXIMUM_REUSE_STRATEGY.md** - Philosophy
  - Why 95% reuse?
  - Win-win-win explained
  - Smart laziness principle

- **ACTION_PLAN_JUSTINE.md** - Strategic pivot
  - Why 2 layers wasn't enough
  - Decision to use llama2.c
  - Implementation approach

- **LLAMA2_PORT_ANALYSIS.md** - Technical analysis
  - Code sections to change
  - Memory requirements
  - Port strategy

#### **Origin Stories**
- **ORIGIN_STORY.md** - Where gpt_nano.h came from
  - 40% llm.c concepts
  - 50% bare-metal adaptations
  - 10% AI assistant fixes

- **JUSTINE_GUIDE.md** - QEMU screencast guide
- **README_JUSTINE.md** - Quick reference

### üß™ **Legacy / Archive (Previous Work)**

| File | Purpose | Status |
|------|---------|--------|
| **gpt_nano.h** | Custom 2-layer GPT (120K params) | ‚ö†Ô∏è SUPERSEDED |
| **llm_chatbot.c** | Interactive chatbot demo | ‚ö†Ô∏è OLD |
| **llm_efi.c** | Original EFI wrapper | ‚ö†Ô∏è OLD |
| **llm_tiny.h** | Minimal transformer | ‚ö†Ô∏è OLD |
| **trained_weights.h** | Nano GPT weights (embedded) | ‚ö†Ô∏è OLD |
| **nano_gpt_weights.bin** | Binary weights | ‚ö†Ô∏è OLD |

### üîß **Build & Test Scripts**

| File | Purpose |
|------|---------|
| **Makefile** | Main build system (use this!) |
| **test-llama2-qemu.ps1** | Windows QEMU test |
| **test-qemu-boot.ps1** | Boot verification |
| **test-qemu.sh** | Linux QEMU test |
| **build-windows.ps1** | Windows build helper |
| **run-qemu.ps1** | QEMU launcher |

### üìã **Other Docs**

| File | Content |
|------|---------|
| **SESSION_SUMMARY.md** | Full session recap |
| **TRAINING_5000_COMPLETE.md** | Training results (5000 steps) |
| **SUCCESS_REPORT.md** | Early milestone |
| **ROADMAP.md** | Future plans |
| **README.md** | Original readme |

---

## üèóÔ∏è How to Build

### Prerequisites (Ubuntu/WSL)
```bash
sudo apt install build-essential gnu-efi mtools qemu-system-x86 ovmf
```

### Build LLaMA2 EFI
```bash
cd llm-baremetal
make llama2.efi          # Compile (outputs llama2.efi)
make llama2-disk         # Create bootable disk with model
make test-llama2         # Test in QEMU
```

### Quick Test
```bash
qemu-system-x86_64 \
  -bios /usr/share/ovmf/OVMF.fd \
  -drive format=raw,file=llama2-disk.img \
  -m 512M -serial mon:stdio
```

---

## üìä Project Evolution

### Phase 1: Custom Nano GPT ‚ö†Ô∏è SUPERSEDED
- Built custom 2-layer GPT (120K params)
- Trained on Tiny Shakespeare
- **Problem**: Too small, novel architecture hard to review

### Phase 2: Strategic Pivot üéØ
- **Justine's feedback**: "2 layers isn't enough, use llama2.c"
- **Decision**: 95% code reuse from proven implementation
- **Philosophy**: Win-win-win (Karpathy + Meta + Us)

### Phase 3: LLaMA2 Port ‚úÖ CURRENT
- **Code**: 95% Karpathy's llama2.c (MIT)
- **Model**: stories15M (15M params, 6 layers)
- **Status**: Compiled, bootable, ready for testing

---

## üéØ Key Achievements

1. ‚úÖ **15M parameter model on bare-metal EFI**
2. ‚úÖ **95% code reuse** from battle-tested llama2.c
3. ‚úÖ **Clear attribution** (Karpathy + Meta)
4. ‚úÖ **Easy to review** (~50 changed lines out of 674)
5. ‚úÖ **Proper documentation** (3 README files)
6. ‚úÖ **Win-win-win** achieved per Justine's wisdom

---

## üìà Metrics

| Metric | Custom Nano GPT | LLaMA2 Port |
|--------|----------------|-------------|
| **Layers** | 2 ‚ùå | 6 ‚úÖ |
| **Parameters** | 120K | 15M |
| **Architecture** | Novel | Proven (Meta) |
| **Code Source** | Mixed | 95% Karpathy |
| **Attribution** | Unclear | Crystal clear |
| **Review Effort** | High | Low (~50 lines) |
| **Trust Level** | "?" | "It's llama2.c!" |

---

## üîç What Changed for EFI (< 5%)

### 1. Static Allocation (Lines 95-140)
```c
// BEFORE: s->x = calloc(p->dim, sizeof(float));
// AFTER:  static float static_x[MAX_DIM]; s->x = static_x;
```

### 2. Simple RNG (Lines 30-45)
```c
// Added LCG (no stdlib)
static uint32_t rng_state = 12345;
uint32_t rand_efi(void) { ... }
```

### 3. EFI File I/O (Lines 390-480)
```c
// BEFORE: open() + mmap()
// AFTER:  EFI_FILE_PROTOCOL + Read()
```

### 4. Console Output (Lines 520-570)
```c
// BEFORE: printf()
// AFTER:  Print(L"...")
```

### What's UNCHANGED: 95%
- ‚úÖ All transformer logic (rmsnorm, softmax, matmul, forward)
- ‚úÖ Multi-head attention
- ‚úÖ RoPE positional encoding
- ‚úÖ KV cache
- ‚úÖ FFN with SwiGLU
- ‚úÖ Sampling (argmax, sample)

---

## üôè Credits

### Primary (95% of code)
1. **Andrej Karpathy** - llama2.c implementation
   - https://github.com/karpathy/llama2.c
   - License: MIT

2. **Meta Platforms** - LLaMA2 architecture
   - Research: "LLaMA: Open and Efficient Foundation Language Models"

### EFI Port (5% modifications)
- **Djibril Diop** - Bare-metal adaptations

### Inspiration
- **Justine Tunney** - Critical feedback & strategy guidance

---

## üöÄ Repository

**GitHub**: https://github.com/djibydiop/llm-baremetal

**Recent Commits**:
```
f679bfa - üìù Summary for Justine: Mission accomplished
0ec0afe - üìä Status Report: LLaMA2 port COMPLETE
dae4374 - üöÄ LLaMA2 bare-metal port COMPLETE (95% Karpathy code)
07ebeeb - üéØ Maximum code reuse strategy
```

---

## üìñ Recommended Reading Order

### New Reviewers
1. **SUMMARY_FOR_JUSTINE.md** (5 min) - Quick overview
2. **README_LLAMA2.md** (10 min) - Full guide
3. **llama2_efi.c** (review ~50 changed lines) - Code

### Developers
1. **README_LLAMA2.md** - Build instructions
2. **Makefile** - Build targets
3. **llama2_efi.c** - Implementation

### Strategy/Philosophy Fans
1. **MAXIMUM_REUSE_STRATEGY.md** - Win-win-win
2. **ACTION_PLAN_JUSTINE.md** - Strategic pivot
3. **ORIGIN_STORY.md** - Project history

---

## üéä Bottom Line

**We built a 15M parameter LLaMA2 model running on bare-metal EFI by standing on giants' shoulders (95% Karpathy's code) with clear attribution. Win-win-win achieved!**

**Status**: ‚úÖ READY FOR TESTING

---

*"Smart people are lazy. Use proven code. Everyone wins." - Justine Tunney*
