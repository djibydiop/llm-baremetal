# ðŸš€ Multimodal LLM Bare-Metal Bootloader

**3 transformer architectures running directly on UEFI firmware - no OS required.**

## Features

- ðŸ”¥ **Multimodal**: 3 models (60MB â†’ 440MB) with auto-detection
- ðŸš€ Bare-metal UEFI implementation (no operating system)
- ðŸ§  stories15M (15M) / NanoGPT (124M) / TinyLlama-Chat (1.1B)
- ðŸŽ¯ Interactive REPL with model-specific prompts
- ðŸ“¦ Dynamic memory allocation (110MB â†’ 1GB)
- âš¡ Optimized with Justine Tunney's powf() (ULP 0.82)

## Supported Models

| Model | Size | Arch | Use Case | Speed |
|-------|------|------|----------|-------|
| **stories15M** | 60MB | 6L/288D | Story generation | ~10 tok/s |
| **NanoGPT-124M** | 48MB | 12L/768D | Text completion | ~3 tok/s |
| **TinyLlama-Chat** | 440MB | 22L/2048D | Conversational | ~0.5 tok/s |

*Speed benchmarks on bare metal Intel Core i5 (no GPU)*

## Quick Start

### 1. Download Models

```bash
# Interactive Python downloader
python3 download_models.py

# Or manual download (stories15M only)
wget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin
wget https://github.com/karpathy/llama2.c/raw/master/tokenizer.bin
```

### 2. Build Bootloader

```bash
# Clean and compile multimodal bootloader
make clean
make

# Create disk image (auto-includes available models)
make llama2-disk
```

### 3. Test

```bash
# QEMU (keyboard input doesn't work - auto-demo only)
qemu-system-x86_64 -bios /usr/share/ovmf/OVMF.fd \
                   -drive format=raw,file=llama2-disk.img \
                   -m 512M

# Real hardware (keyboard works!)
sudo dd if=llama2-disk.img of=/dev/sdX bs=4M
# Boot from USB
```

## Prerequisites

**Linux/macOS/WSL:**
- gcc, binutils (ld, objcopy)
- gnu-efi development headers
- QEMU with OVMF firmware
- mtools (for FAT filesystem)

**Installation:**
```bash
# Ubuntu/Debian
sudo apt install build-essential gnu-efi qemu-system-x86 ovmf mtools

# Fedora
sudo dnf install gcc binutils gnu-efi-devel qemu-system-x86 edk2-ovmf mtools

# macOS (via Homebrew)
brew install qemu
# Note: May need to install gnu-efi from source
```

## Model Weights & Tokenizer

The setup script automatically downloads from HuggingFace:

- `stories15M.bin` (60MB) - Model weights
- `tokenizer.bin` (424KB) - BPE vocabulary

Source: <https://huggingface.co/karpathy/tinyllamas>

Alternatively, download manually and place both files in the project root.

## Technical Details

**Architecture**: LLaMA2 (Meta)  
**Implementation**: Based on Karpathy's llama2.c (MIT license)  
**Environment**: UEFI firmware via gnu-efi  
**Binary Size**: ~70KB  
**Runtime Memory**: ~20MB  
**Generation**: 200 tokens with temperature=0.9

## Code Structure

- `llama2_efi.c` - Main implementation (932 lines)
  - Transformer inference engine
  - BPE tokenizer loader
  - Temperature sampling implementation
- `Makefile` - Build configuration
- `setup.sh` - Setup and model download script
- `startup.nsh` - UEFI auto-boot script

## How It Works

1. UEFI firmware loads the EFI application
2. Application reads model weights (60MB) and tokenizer (424KB) from disk
3. Allocates buffers for activations and KV cache
4. Runs transformer forward pass for each token
5. Applies temperature sampling for next token selection
6. Decodes token IDs to readable text using BPE vocabulary
7. Generates 200-token sequences

## Example Output

```text
Once upon a time, there was a little girl named Lily. She loved to play in the park
with her friends. One day, Lily saw a big red ball bouncing down the hill...
```

The model generates coherent short stories using temperature sampling (temp=0.9) for natural, varied text.

## Status

âœ… **Fully Functional**:

- Model inference and token generation
- BPE tokenizer with readable text output
- Temperature sampling for high-quality generation
- 200-token story generation

ðŸš§ **Future Enhancements**:

- User-provided prompts
- Configurable temperature/top-p parameters
- Hardware testing (currently validated on QEMU/OVMF)

## License

MIT License - see source code header for full details
