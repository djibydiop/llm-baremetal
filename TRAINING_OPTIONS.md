# Options d'AmÃ©lioration stories15M

## ğŸ“Š ModÃ¨le Actuel
- **Fichier**: stories15M.bin (60 MB)
- **Architecture**: dim=288, 6 layers, 6 heads
- **Vocabulaire**: 32000 tokens
- **Performance**: âœ… Fonctionne en bare-metal
- **QualitÃ©**: Texte basique mais fonctionnel

## ğŸ¯ Options d'AmÃ©lioration

### Option 1: Fine-tuning stories15M â­ RECOMMANDÃ‰
**Avantages:**
- âœ… Garde la taille (60 MB)
- âœ… Compatible bare-metal garanti
- âœ… Temps raisonnable (2-3 heures GPU)
- âœ… Meilleure cohÃ©rence de texte

**Commandes:**
```bash
cd llm-baremetal
wsl bash -c "source venv/bin/activate && python3 train_stories15m.py --finetune"
```

**ParamÃ¨tres:**
- Plus d'Ã©poques sur TinyStories
- Learning rate rÃ©duit (1e-4)
- Batch size optimisÃ©
- Validation loss tracking

---

### Option 2: EntraÃ®ner stories42M
**Avantages:**
- ğŸš€ Texte beaucoup plus cohÃ©rent
- ğŸ“š Meilleure comprÃ©hension contextuelle
- ğŸ¨ Vocabulaire plus riche

**InconvÃ©nients:**
- â° Temps long (6-8 heures GPU)
- ğŸ’¾ Taille plus grande (~150 MB)
- ğŸ”§ Peut nÃ©cessiter ajustements bare-metal

**Architecture:**
- dim=512
- 8 layers
- 8 heads
- Vocab: 32000

---

### Option 3: Utiliser NanoGPT-124M (DÃ©jÃ  tÃ©lÃ©chargÃ©)
**Avantages:**
- âœ… DÃ©jÃ  prÃ©sent (nanogpt.bin, 472 MB)
- ğŸ¯ GPT-2 architecture Ã©prouvÃ©e
- ğŸ“– Pas besoin d'entraÃ®nement

**InconvÃ©nients:**
- ğŸ’¾ Fichier lourd (472 MB)
- â±ï¸ Chargement plus long en bare-metal
- ğŸ”§ NÃ©cessite code multi-modÃ¨les

**Test rapide:**
```bash
# Modifier select_model() pour charger NanoGPT
# Recompiler et tester
```

---

### Option 4: EntraÃ®nement from Scratch
**Avantages:**
- ğŸ¯ ContrÃ´le total du dataset
- ğŸ”§ Personnalisation complÃ¨te
- ğŸ“š Dataset TinyStories complet

**InconvÃ©nients:**
- â° Temps long (4-5 heures GPU)
- ğŸ’» NÃ©cessite bon GPU
- ğŸ”¬ RÃ©sultats variables

---

## ğŸ“‹ Script d'EntraÃ®nement Fine-tuning

CrÃ©er `train_stories15m_finetune.py`:

```python
import torch
from torch.nn import functional as F
from dataclasses import dataclass
import numpy as np

@dataclass
class ModelConfig:
    dim: int = 288
    n_layers: int = 6
    n_heads: int = 6
    vocab_size: int = 32000
    max_seq_len: int = 256

# Configuration fine-tuning
learning_rate = 1e-4  # RÃ©duit pour fine-tuning
batch_size = 64
max_iters = 50000
eval_interval = 1000

# Charger le modÃ¨le existant
checkpoint = torch.load('stories15M.pt')
model.load_state_dict(checkpoint['model'])

# Dataset TinyStories
# ... (code de chargement)

# Boucle d'entraÃ®nement amÃ©liorÃ©e
for iter in range(max_iters):
    # Training step
    optimizer.zero_grad()
    logits = model(x)
    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))
    loss.backward()
    optimizer.step()
    
    # Validation
    if iter % eval_interval == 0:
        val_loss = evaluate()
        print(f"Iter {iter}: train_loss={loss:.4f}, val_loss={val_loss:.4f}")
        
        # Save checkpoint
        if val_loss < best_val_loss:
            torch.save({
                'model': model.state_dict(),
                'iter': iter,
                'val_loss': val_loss
            }, 'stories15M_finetuned.pt')

# Export to .bin
python3 export.py stories15M_finetuned.pt --output=stories15M_improved.bin
```

---

## ğŸš€ Plan d'Action RecommandÃ©

### Phase 1: Test Rapide (maintenant)
1. Tester NanoGPT-124M existant (472 MB)
2. VÃ©rifier si Ã§a boot correctement
3. Comparer qualitÃ© vs stories15M

### Phase 2: Fine-tuning (2-3 heures)
1. Cloner llama2.c si pas dÃ©jÃ  fait
2. TÃ©lÃ©charger TinyStories complet
3. Lancer fine-tuning stories15M
4. Exporter vers .bin
5. Tester en bare-metal

### Phase 3: Ã‰ventuel Upgrade (si nÃ©cessaire)
1. Si fine-tuning insuffisant â†’ stories42M
2. EntraÃ®ner modÃ¨le plus grand
3. Optimiser pour bare-metal

---

## ğŸ’» Commandes Rapides

### Tester NanoGPT maintenant:
```bash
# Modifier llama2_efi.c pour utiliser nanogpt.bin
# Rebuild
make clean && make
make test-image
./test-qemu.ps1
```

### Lancer Fine-tuning:
```bash
wsl
cd /mnt/c/Users/djibi/Desktop/yama_oo/yama_oo/llm-baremetal
git clone https://github.com/karpathy/llama2.c.git
cd llama2.c
python3 tinystories.py download
python3 train.py --finetune=../stories15M.bin --max_iters=50000 --learning_rate=1e-4
python3 export.py --checkpoint=out/model.pt --output=../stories15M_improved.bin
```

---

## â“ Quelle Option Choisir?

**Tu as un GPU ?**
- âœ… Oui â†’ Option 1 (fine-tuning) ou 2 (stories42M)
- âŒ Non â†’ Option 3 (NanoGPT dÃ©jÃ  prÃªt)

**Tu veux quoi ?**
- ğŸ¯ Meilleure qualitÃ©, taille OK â†’ Fine-tuning (Option 1)
- ğŸš€ Max qualitÃ©, taille pas grave â†’ stories42M (Option 2)
- âš¡ Rapide, test maintenant â†’ NanoGPT (Option 3)
- ğŸ”¬ ExpÃ©rimentation complÃ¨te â†’ From scratch (Option 4)

---

## ğŸ“Š Comparaison Finale

| Option | Temps | Taille | QualitÃ© | DifficultÃ© |
|--------|-------|--------|---------|------------|
| Fine-tune | 2-3h | 60MB | â­â­â­â­ | Facile |
| stories42M | 6-8h | 150MB | â­â­â­â­â­ | Moyen |
| NanoGPT | 0h | 472MB | â­â­â­â­â­ | Facile |
| From scratch | 4-5h | 60MB | â­â­â­â­ | Difficile |

**Ma recommandation: Commence par tester NanoGPT (0 temps), puis fine-tune si besoin!**
